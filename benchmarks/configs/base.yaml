# Base configuration for GPT-OSS-120B benchmarks
# This file contains common settings shared across all service configurations
# Service-specific configs inherit from this and override only what's different

# API and model settings
tokenizer: "openai/gpt-oss-120b"
hf_token: "${HF_TOKEN}"  # Use environment variable for security

# ---------------
# STEP 1: DATASET GENERATION
# ---------------
# Dataset config
dataset_dir: "./output/dataset_gpt-oss-120"
prompt_type: "synthetic_multiturn"  # Options: synthetic_multiturn, synthetic_shared, sharegpt, client_trace
dataset_configs:
  synthetic_multiturn:
    shared_prefix_length: 0
    prompt_length: 500
    prompt_std: 150
    num_turns: 3.55
    num_turns_std: 2.89
    num_sessions: 10000
    num_sessions_std: 1

  synthetic_shared:
    prompt_length: "3997,5868,2617"
    prompt_std: "17,28,1338"
    shared_prop: "0.95,0.97,0.03"
    shared_prop_std: "0.00001,0.00001,0.00001"
    num_samples: "12,8,79"
    num_prefix: "1,1,1"
    num_dataset_configs: 3

  sharegpt:
    target_dataset: "/tmp/ShareGPT_V3_unfiltered_cleaned_split.json"

  client_trace:
    trace: ${client_trace_path}

# ---------------
# STEP 2: WORKLOAD GENERATION
# ---------------
# Workload config
dataset_file: ${dataset_dir}/${prompt_type}.jsonl
workload_type: "constant"  # Options: constant, synthetic, stat, azure, mooncake
interval_ms: 1000
duration_ms: 120000
workload_dir: "./output/workload_gpt-oss-120b/${workload_type}"

workload_configs:
  synthetic:
    use_preset_pattern: true 
    preset_patterns:
      traffic_pattern: 'slow_rising'
      prompt_len_pattern: 'severe_fluctuation'
      completion_len_pattern: 'slight_fluctuation'
    pattern_files:
      traffic_file: "scenarios/autoscaling/workload-configs/predefined/traffic-configs/HighSlow.json"
      prompt_len_file: "scenarios/autoscaling/workload-configs/predefined/prompt-len-configs/HighSlow.json"
      completion_len_file: "scenarios/autoscaling/workload-configs/predefined/completion-len-configs/HighSlow.json"
      max_concurrent_sessions: 2

  constant:
    target_qps: 50
    target_prompt_len: null
    target_completion_len: null
    max_concurrent_sessions: 2

  stat:
    traffic_file: ${stat_describe_traffic}
    prompt_len_file: ${stat_describe_prompt_len}
    completion_len_file: ${stat_describe_completion_len}
    stat_trace_type: "cloudide" 
    qps_scale: 1.0
    output_scale: 1.0
    input_scale: 1.0

  azure:
    trace_path: "/tmp/AzureLLMInferenceTrace_conv.csv"

  mooncake:
    trace_path: "/tmp/Mooncake_trace.jsonl"
    trace_type: "conversation"

# ---------------
# STEP 3: CLIENT DISPATCH
# ---------------
# Client and trace analysis output directories
workload_file: "./output/workload_gpt-oss-120b/${workload_type}/workload.jsonl"
# Service-specific: endpoint, api_key, target_model, client_output, trace_output
endpoint: ""  # Override in service configs
api_key: "${API_KEY}"  # Use environment variable
target_model: "gpt-oss-120b"
time_scale: 1.0
routing_strategy: "random" 
streaming_enabled: true
output_token_limit: 10000
timeout_second: 60.0
max_retries: 0
client_duration_limit: null
client_max_concurrent_sessions: 1
client_max_requests: null

# ---------------
# OPTIONAL: ANALYSIS
# ---------------
# Service-specific: trace_output
trace_output: "./output/trace_analysis"
goodput_target: "tpot:0.5"

